---
layout: post
categories: posts
title: "[BERT] BERT란 무엇인가 - 기본"
comments: true
---


## [ BERT란? ] 
BERT(버트, Bidirectional Encoder Representations from Transformers)는 2018년 11월,구글이 공개한 인공지능(AI) 언어모델이다. 자연어를 이해(Natural Language Understanding)하기 위한 모델로 사용되며, 이름에서도 알 수 있듯 Transformer 모델의 Encoder를 사용한 양방향 학습 모델이다.
 
BERT는 특히 자연어 처리에서 임베딩(Embedding)을 할 때 사용하는 사전 훈련 언어모델(Pre-training Language Model)로 주로 사용되고 있다. BERT가 등장하기 이전에는 Word2Vec, Glove, Fasttext, ELMo 등의 방식을 임베딩에 많이 사용했지만 요즘 고성능을 내는 대부분의 모델에서 BERT를 사용하고 있다고 한다.


### > 참고 - Transformer(self-attention)
역시 구글에서 2017년에 발표한 논문인 "Attention is all you need"에서 나온 모델로 기존의 seq2seq 구조인 인코더(Encoder)-디코더(Decoder) 구조를 따르면서도, 논문의 이름처럼 어텐션(Attention)만으로 구현한 모델이다 [3,5]. 즉 RNN, LSTM 등의 셀을 일체 사용하지 않고 Attention 구조만으로 time sequence를 인식하여 학습할 수 있는 모델이라는 뜻이다. 현재는 거의 모든 SOTA(state-of-the-art) 모델들(BERT, GPT등)이 Transformer 구조에 기반하여 구현되고 있다.

일반적인 Seq2Seq-Attention 모델에서도 원본 언어(Source Language)와 번역된 언어(Target Language)간의 대응 여부는 Attention을 통해 찾을 수 있었다. 하지만 각각 자신 언어만의 관계에 대해서는 나타낼 수 없었다. 예를 들면, <b><u>"I love tiger but it is scare"</u></b>와 <b><u>"나는 호랑이를 좋아하지만 그것(호랑이)는 무섭다"</u></b> 사이의 관계는 Attention을 통해 매칭이 가능했지만, <b><u>'it'</b></u>이 무엇을 나타내는지와 같은 문제는 기존 인코더-디코더 기반의 어탠션 매커니즘에서는 찾을 수 없었다. 하지만 Transformer 구조를 이용하여 위의 문제를 해결할 수 있게 되었다 [1, 2].


<p align="center">
	<img src="../../assets/img/post/bert_transformer.png" alt="transformer img"/>
</p>


Transformer 모델의 구조도 내에서 각 부분이 담당하는 역할은 다음과 같다.

- 빨간색 : 인코더(Encoder)
- 파란색 : 디코더(Decoder)
- 보라색 : 인코더에서 Self-Attention이 일어나는 부분
- 하늘색 : 디코더에서 Self-Attention이 일어나는 부분
- 겨자색 : 인코더와 디코더의 Attention이 일어나는 부분


## [ BERT 사용 방식 ]
우선 아래의 그림[7] 에서 볼 수 있듯, 실제로 해결하고자 하는 문제와 관련된 대량의 코퍼스(*corpus, 말뭉치)를 이용하여 BERT 모델을 학습한다. 그리고 Pre-trained BERT 모델과 결합된 예측 모델을 이용하여 실제 문제에서 사용되는 데이터로 학습(transfer learning) 시킨다. 이렇게 학습한 최종 모델을 이용하여 원하는 예측을 진행할 수 있다.

<p align="center">
	<img src="../../assets/img/post/bert_process.png" alt="bert process img"/>
</p>


### > 참고 - Transfer Learning 방식
이러한 전이 학습 방식은 크게 두 가지로 나눌 수 있다. 하나는 아래 그림의 (a)와 같은 특성 추출(feature extraction) 방법이고, 다른 하나는 (b), (c)와 같은 미세 조정(fine tuning) 방법이다. 

특성 추출 방법은 데이터의 특성을 추출하는 부분의 네트워크를 고정하고 결과를 출력하는 계층인 예측 모델 계층을 해결하려는 문제에 맞도록 새롭게 정의하여 예측 모델 계층만 학습시키는 것이다. 

미세 조정 방법은 특성 추출을 할 때 고정했던 네트워크의 전체(b) 또는 일부(c)를 고정에서 해제하여 새롭게 학습시키는 것을 말한다. 이때 주의할 점은 특성 추출을 이용해서 예측 모델 계층을 먼저 훈련시킨 뒤, 미세 조정을 진행해야 한다는 점이다. 만약 그렇지 않으면 예측 모델 계층이 훈련되는 동안 너무 큰 오차 신호가 다른 네트워크로 전파되기 때문이다.
 
<p align="center">
	<img src="../../assets/img/post/bert_transfer_learning.png" alt="transfer learning"/>
</p>



## [ BERT의 특징 ]
BERT의 Input은 아래의 그림처럼 Token Embedding + Segment Embedding + Position Embedding 으로 이루어진다 [1, 2].

<p align="center">
	<img src="../../assets/img/post/bert_input.png" alt="bert input img"/>
</p>


### Token Embedding
...

### Segment Embedding
...

### Position Embedding
...



## [ BERT 학습 방법 ]
BERT는 언어의 특성을 잘 학습하도록 MLM(Masked Language Model)과 NSP(Next Sentence Prediction) 방식을 이용하여 Pre-Training을 진행한다.

### MLM(Masked Language Model)
...

### NSP(Next Sentence Prediction)
...

### > 참고 - 문맥 의존 방법(단방향 또는 얕은 양방향, 깊은 양방향) vs 문맥에 의존하지 않는 방법 
[8]...



## [ 이후에 나온 다른 Pre-trained 모델 ]
- GPT
- BART
- ELECTRA
- 등등
- [6]...

<p align="center">
	<img src="../../assets/img/post/bert_pretrained_model_survey.png" alt="nlp pre-trained model survey img"/>
</p>


## [ Future work ]
- BERT의 내부 특징 학습 및 정리
- BERT의 Pre-training 방법 학습 및 정리
- NLP Pre-trained 모델 survey 논문 읽고 정리
 

## Reference
- [1] [BERT 논문 - BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/pdf/1706.03762.pdf)
- [2] [BERT 설명 국문 블로그](https://ebbnflow.tistory.com/151)
- [3] [Transformer(self-attention) 논문 - Attention Is All You Need](https://arxiv.org/pdf/1706.03762.pdf)
- [4] [Transformer(self-attention) 설명 국문 블로그 1](https://medium.com/platfarm/%EC%96%B4%ED%85%90%EC%85%98-%EB%A9%94%EC%BB%A4%EB%8B%88%EC%A6%98%EA%B3%BC-transfomer-self-attention-842498fd3225)
- [5] [Transformer(self-attention) 설명 국문 블로그 2](https://wikidocs.net/31379)
- [6] [NLP pre-trained model survey 논문 - Pre-trained models for natural language processing](https://link.springer.com/article/10.1007/s11431-020-1647-3)
- [7] [bert 관련 이미지](http://jalammar.github.io/illustrated-bert/)
- [8] [인공지능 신문 - BERT](http://www.aitimes.kr/news/articleView.html?idxno=13117)