---
layout: post
categories: posts
title: "[BERT] BERT란 무엇인가 - 기본"
aside:
 toc: true
comments: true
---

이 문서에서는 BERT에 대한 기본 내용을 설명한다. 또한 이 문서에 포함되어 있는 내용은 작성일(2021-02-24) 기준 내용이므로 참고하길 바란다.


## [ BERT란? ]
BERT(버트, Bidirectional Encoder Representations from Transformers)는 2018년 11월,구글이 공개한 인공지능(AI) 언어모델이다. 자연어를 이해(Natural Language Understanding)하기 위한 모델로 사용되며, 이름에서도 알 수 있듯 Transformer 모델의 Encoder를 사용한 양방향 학습 모델이다.
 
BERT는 특히 자연어 처리에서 임베딩(Embedding)을 할 때 사용하는 사전 훈련 언어모델(Pre-training Language Model)로 주로 사용되고 있다. BERT가 등장하기 이전에는 Word2Vec, Glove, Fasttext, ELMo 등의 방식을 임베딩에 많이 사용했지만 요즘 고성능을 내는 대부분의 모델에서 BERT를 사용하고 있다고 한다.


### > 참고 - Transformer(self-attention)
역시 구글에서 2017년에 발표한 논문인 "Attention is all you need"에서 나온 모델로 기존의 seq2seq 구조인 인코더(Encoder)-디코더(Decoder) 구조를 따르면서도, 논문의 이름처럼 어텐션(Attention)만으로 구현한 모델이다 [3,5]. 즉 RNN, LSTM 등의 셀을 일체 사용하지 않고 Attention 구조만으로 time sequence를 인식하여 학습할 수 있는 모델이라는 뜻이다. 현재는 거의 모든 SOTA(state-of-the-art) 모델들(BERT, GPT등)이 Transformer 구조에 기반하여 구현되고 있다.

일반적인 Seq2Seq-Attention 모델에서도 원본 언어(Source Language)와 번역된 언어(Target Language)간의 대응 여부는 Attention을 통해 찾을 수 있었다. 하지만 각각 자신 언어만의 관계에 대해서는 나타낼 수 없었다. 예를 들면, <b><u>"I love tiger but it is scare"</u></b>와 <b><u>"나는 호랑이를 좋아하지만 그것(호랑이)는 무섭다"</u></b> 사이의 관계는 Attention을 통해 매칭이 가능했지만, <b><u>'it'</u></b>이 무엇을 나타내는지와 같은 문제는 기존 인코더-디코더 기반의 어탠션 매커니즘에서는 찾을 수 없었다. 하지만 Transformer 구조를 이용하여 위의 문제를 해결할 수 있게 되었다 [1, 2].


<p align="center">
	<img src="../../assets/img/post/bert_transformer.png" alt="transformer img"/>
</p>


Transformer 모델의 구조도 내에서 각 부분이 담당하는 역할은 다음과 같다.

- 빨간색 : 인코더(Encoder)
- 파란색 : 디코더(Decoder)
- 보라색 : 인코더에서 Self-Attention이 일어나는 부분
- 하늘색 : 디코더에서 Self-Attention이 일어나는 부분
- 겨자색 : 인코더와 디코더의 Attention이 일어나는 부분


## [ BERT로 풀 수 있는 문제 유형 ]
BERT 모델은 대표적인 11개의 NLP 문제에 대해서 좋은 성능을 나타냈다. 대표적인 문제 유형을 논문에서는 (a)Sentence Pair Classification Task, (b)Single Sentence Classification, (c)Question Answering Task, (d)Single Sentence Tagging Tasks 이렇게 4가지로 나누었다. 

<p align="center">
	<img src="../../assets/img/post/bert_problem.png" alt="bert solved problem"/>
</p>

이러한 문제를 모델의 Input 형태와 따라 한번 더 구분해보면 다음과 같이 나눌 수 있다.

- 한 문장을 Input으로 결과를 낼 수 있는 문제
    1. 하나의 문장 분류(single sentence classification)
        - SST-2(Stanford Sentiment Treebank) : 영화 리뷰인 한 문장이 주어졌을 때, 감성 분류
        - CoLA(Corpus of Linguistic Acceptability) : 영어 문장이 언어학적으로 적합한지 이진 분류 예측
        - 외에도 스팸 메일 찾기, 문서의 카테고리 분류 등의 문제가 여기에 속한다 [13].
    2. 문장 내 단어 라벨링(single sentence tagging task)
        - CoNLL-2003 NER(Conference on Computational Natural Language Learning - 2003 Named Entity Recognition) : 한 문장이 주어졌을 때 네 가지 유형의 개체명(사람, 위치, 조직, 기타 항목)을 중심으로 개체명 예측
        - 외에도 긴 이름의 first/middle/last name을 라벨로 예측하거나 품사 태깅(POS Tagging, Part-of-Speech Tagging) 등의 문제가 여기에 속한다 [13].
- 한 쌍의 문장으로 Input으로 결과를 낼 수 있는 문제
    1. 문장 두 개의 관계 분류(sentence pair classification)
        - MNLI(Multi-Genre Natural Language Inference) : 한 쌍의 문장이 주어졌을 때, 두 번째 문장이 첫 번째 문장에 대해서 수반(entailment)/상충(contradiction)/중립(neutral)되는지 분류 예측
        - QQP(Quora Question Pairs) : Quora에 올라온 두 개의 질문이 의미론적으로 동일한지 확인하는 것이 목표인 이진 분류 예측
        - QNLI(Question Natural Language Inference) : QA 문제인 SQuAD 문제를 이진 분류 문제로 변환한 것이다. 문단(paragraph)이 있을때 주어진 질문(question)에 대하여 올바른 정답을 할 수 있을지 이진 분류 예측
        - STS-B(Seemantic Textual Similarity Benchmark) : 두 문장이 의미론적으로 얼마나 유사한지 1부터 5까지의 점수로 나타내는 분류 예측 문제
        - MRPC(Microsoft Research Paraphrate Corpus) : 두 문장의 의미론적으로 유사한지 이진 분류 예측 
        - RTE(Recognizing Textual Entailment) : MNLI와 유사한 이진 분류 예측, 하지만 데이터가 훨씬 적음
        - SWAG(The Situations With Adversarial Generations) : 한 문장이 주어졌을 때, 네 가지의 선택지 중에서 가장 잘 이어지는 문장을 찾는 문제
    2. 묻고 답하기(question & answering)
        - SQuAD v1.1(Stanford Question Answering Dataset) : 질문(question)과 문단(paragraph)이 주어졌을 때 문단 속에 있는 질문에 대한 답변(answer)의 위치를 예측, 이 문제를 읽기 이해(MRC, Machine Reading Comprehension)라고 할 수 있다.


## [ BERT 사용 방식 ]
우선 아래의 그림[7] 에서 볼 수 있듯, 실제로 해결하고자 하는 문제와 관련된 대량의 코퍼스(*corpus, 말뭉치)를 이용하여 BERT 모델을 학습한다. 그리고 Pre-trained BERT 모델과 결합된 예측 모델을 이용하여 실제 문제에서 사용되는 데이터로 학습(transfer learning) 시킨다. 이렇게 학습한 최종 모델을 이용하여 원하는 예측을 진행할 수 있다.

<p align="center">
	<img src="../../assets/img/post/bert_process.png" alt="bert process img"/>
</p>


### > 참고 - Transfer Learning 방식
이러한 전이 학습 방식은 크게 두 가지로 나눌 수 있다. 하나는 아래 그림의 (a)와 같은 특성 추출(feature extraction) 방법이고, 다른 하나는 (b), (c)와 같은 미세 조정(fine tuning) 방법이다. 

특성 추출 방법은 데이터의 특성을 추출하는 부분의 네트워크를 고정하고 결과를 출력하는 계층인 예측 모델 계층을 해결하려는 문제에 맞도록 새롭게 정의하여 예측 모델 계층만 학습시키는 것이다. 

미세 조정 방법은 특성 추출을 할 때 고정했던 네트워크의 전체(b) 또는 일부(c)를 고정에서 해제하여 새롭게 학습시키는 것을 말한다. 이때 주의할 점은 특성 추출을 이용해서 예측 모델 계층을 먼저 훈련시킨 뒤, 미세 조정을 진행해야 한다는 점이다. 만약 그렇지 않으면 예측 모델 계층이 훈련되는 동안 너무 큰 오차 신호가 다른 네트워크로 전파되기 때문이다.
 
<p align="center">
	<img src="../../assets/img/post/bert_transfer_learning.png" alt="transfer learning"/>
</p>



## [ BERT의 특징 ]
BERT의 Input은 아래의 그림처럼 Token Embedding + Segment Embedding + Position Embedding 으로 이루어진다 [1, 2].

<p align="center">
	<img src="../../assets/img/post/bert_input.png" alt="bert input img"/>
</p>


### Token Embedding
Word Piece Embedding [9] 방식을 사용한다 [2]. (c.f. Byte Pare Encoding, BPE와 비슷) 

만약 모델이 모르는 단어가 등장하면 그 단어를 ‘단어 집합에 없는 단어’ 라는 의미에서 OOV(Out-Of-Vocabulary) 또는 UNK(Unknown Token)이라고 표현한다. Word Piece Embedding은 이처럼 세상의 모든 단어를 학습할 수 없는 문제가 있다. 이를 해결하기 위해 나온 알고리즘이고, 이를 이용하면 OOV나 희귀 단어, 신조어와 같은 문제를 완화시킬 수 있다고 알려져있다 [12].

하나의 단어는 더 작은 단위의 의미있는 여러 서브워드(subword)들의 조합으로 구성된 경우가 많다는 점을 이용해서 하나의 단어를 여러 서브워드로 분리해서 단어를 인코딩 및 임베딩하는 것을 말한다. 서브워드는 자주 등장하면서 가장 긴 길이의 서브워드를 하나의 단위로 만든다 [2].

<b>논문 예시)</b>

- 예시 [9]
    - Word : Jet makers feud over seat width with big orders at stake. 
    - Wordpieces : \_J et \_makers \_fe ud \_over \_seat \_width \_with \_big \_orders \_at \_stake
- 구분 표현
    -  _ : 원래 word를 구분
    - 띄어쓰기 : 하나의 word 내에서 자주 발생하는 subword로 나누는 것을 표현 

<b>다른 예시) </b>

- 예시 [12]
    - vocabulary에 low, lower, newest, widest 라는 단어들이 있을때, lowest라는 단어가 등장하면?
- 적용
    - [low], [low, er], [new, est], [wid, est]로 subword를 분리할 수 있고, lowest는 low + est의 의미로 구분할 수 있게될 것이다.



### Segment Embedding
토큰화(Tokenization) 시킨 단어들을 다시 하나의 문장으로 만드는 작업이다 [2]. 예를 들어 ("I like dog", "I love puppy")라는 한 쌍의 데이터가 있을 때, BERT 모델이 Input에서 각각의 문장인 "I like dog"와 "I love puppy"를 구별할 수 있도록 도와주는 방법이다.

Segment Embedding을 진행하는 과정을 나타내면 아래의 그림과 같다 [14]. 

<p align="center">
	<img src="../../assets/img/post/bert_segment_embedding.png" alt="bert segment embedding img"/>
</p>

이렇게 Segment Embedding을 진행하게 되면 BERT 모델이 서로 다른 문장의 각각의 토큰(token)을 구분할 수 있게 만들어준다. 만약 모델의 Input이 하나 밖에 없는 문제의 경우, 즉 위의 예시에서 모델의 Input이 "I like dog" 문장 밖에 없는 문제의 경우에는 Segment Embedding이 모두 0에 해당하는 벡터로 나타난다 [14]. 



### Position Embedding
어순은 언어를 이해하는데 중요한 정보가 된다. 예를 들어 "A lion chases a deer"라는 문장 각 토큰들의 어순이 무시된다면 "A a deer chases lion"처럼 엉터리 문장도 원래의 문장과 같은 의미로 인식하게 된다.

기존의 RNN류의 모델은 셀과 셀들이 병렬적으로 연결된 구조로, 모델의 구조 자체가 시간의 연속성을 처리할 수 있게 된다. Transformer 모델의 핵심이 되는 Attention 구조는 데이터가 통과하는 layer의 수가 적어 연산에서의 이득을 얻을 수 있고, RNN류 모델의 학습 과정에서 발생하는 기울기 소실/폭발 등에서 자유롭긴 하지만 기존의 자연어 처리에 사용되는 LSTM, RNN 기반의 모델과 달리 '시간의 연속성'을 모델에서 처리하지 못한다 [15]. 

이렇게 모델 구조에서 자연스럽게 sequential information을 얻지 못하는 경우에 어순 정보를 추가하는 방식이 Position Embedding이다. 이러한 방식은 Transoformer류의 모델 외에도 End-to-End Memory Network (MemN2N)등의 모델에서도 사용된다 [16].

<p align="center">
	<img src="../../assets/img/post/bert_position_encoding.png" alt="bert position encoding img"/>
</p>

시퀀스 내에서 단어의 위치를 pos, 모델의 임베딩 차원의 수가 d라고 하고(d 차원), i는 0부터 d까지의 차원에서 특정 차원을 의미한다. 그리고 주기가 ${10000^{2i/d_model}} * {2\pi}$ 이때, i 차원에서 사용되는 함수는 짝수일때는 sin함수, 홀수일때는 cos함수가 사용된다. 

<p align="center">
	<img src="../../assets/img/post/bert_position_encoding_example.png" alt="bert position encoding example img"/>
</p>

하나의 함수를 사용하는 것이 아니라 두 개의 함수를 홀수, 짝수 차원에 따라 사용하는 이유는 다음과 같다. 위치(position)가 커질 때마다 그 값이 다시 작아지는 주기함수의 특징에서, 어떤 특정 두 토큰의 위치 값이 동일해질 수 있는 것을 방지하기 위함이다. 하지만 sin, cos 함수를 각각 짝수와 홀수 차원에 따라 함께 사용하면 값들의 표현을 일정하게 증가/감소 하는 것으로 표현할 수 있다. 즉 sin(x)와 cos(x)를 동시에 사용해야만, sin(x+k)와 cos(x+k)를 선형 변환으로 표현가능하다 [17, 18].

위의 식에 따라 PE(Position Embedding)한 각각의 값은 어순(위치, pos, position)에 따라 -1 ~ +1 사이의 값을 갖게 된다. 

이렇게 Position Embedding된 값은 다른 임베딩 값들과 "더해져서" 사용된다 [19].

<p align="center">
	<img src="../../assets/img/post/bert_position_encoding_example2.png" alt="bert position encoding example2 img"/>
</p>


## [ BERT 학습 방법 ]
BERT는 언어의 특성을 잘 학습하도록 MLM(Masked Language Model)과 NSP(Next Sentence Prediction) 방식을 이용하여 Pre-Training을 진행한다.

### MLM(Masked Language Model)
...

### NSP(Next Sentence Prediction)
...

### > 참고 - 문맥 의존 방법(단방향 또는 얕은 양방향, 깊은 양방향) vs 문맥에 의존하지 않는 방법 [8]
...



## [ 이후에 나온 다른 Pre-trained 모델 ]
- GPT
- BART
- ELECTRA
- 등등
- [6]...

<p align="center">
	<img src="../../assets/img/post/bert_pretrained_model_survey.png" alt="nlp pre-trained model survey img"/>
</p>


## [ Future work ]
- BERT의 내부 특징 학습 및 정리
- BERT의 Pre-training 방법 학습 및 정리
- NLP Pre-trained 모델 survey 논문 읽고 정리
 

## Reference
- [1] [BERT 논문 - BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/pdf/1706.03762.pdf)
- [2] [BERT 설명 국문 블로그 1](https://ebbnflow.tistory.com/151)
- [3] [Transformer(self-attention) 논문 - Attention Is All You Need](https://arxiv.org/pdf/1706.03762.pdf)
- [4] [Transformer(self-attention) 설명 국문 블로그 1](https://medium.com/platfarm/%EC%96%B4%ED%85%90%EC%85%98-%EB%A9%94%EC%BB%A4%EB%8B%88%EC%A6%98%EA%B3%BC-transfomer-self-attention-842498fd3225)
- [5] [Transformer(self-attention) 설명 국문 블로그 2](https://wikidocs.net/31379)
- [6] [NLP pre-trained model survey 논문 - Pre-trained models for natural language processing](https://link.springer.com/article/10.1007/s11431-020-1647-3)
- [7] [bert 관련 이미지](http://jalammar.github.io/illustrated-bert/)
- [8] [인공지능 신문 - BERT](http://www.aitimes.kr/news/articleView.html?idxno=13117)
- [9] [Word Piece Embedding 논문 - Google’s Neural Machine Translation System: Bridging the Gap between Human and Machine Translation](https://arxiv.org/pdf/1609.08144.pdf)
- [10] [Word Piece Embedding 설명 국문 블로그 1](https://ai-information.blogspot.com/2019/02/word-piece-embedding.html)
- [11] [Word Piece Embedding 설명 국문 블로그 2](https://lovit.github.io/nlp/2018/04/02/wpm/)
- [12] [Byte Pair Encoding 설명 국문 블로그](https://wikidocs.net/22592)
- [13] [BERT 설명 국문 블로그 2](https://jiho-ml.com/weekly-nlp-28/)
- [14] [BERT 설명 국문 블로그 3](https://medium.com/@_init_/why-bert-has-3-embedding-layers-and-their-implementation-details-9c261108e28a)
- [15] [BERT Positional Encoding 국문 블로그 1](https://skyjwoo.tistory.com/entry/positional-encoding%EC%9D%B4%EB%9E%80-%EB%AC%B4%EC%97%87%EC%9D%B8%EA%B0%80)
- [16] [BERT Positional Encoding 국문 블로그 2](https://inmoonlight.github.io/2020/01/26/Positional-Encoding/)
- [17] [BERT Positional Encoding 국문 블로그 3](https://pongdangstory.tistory.com/482)
- [18] [BERT Positional Encoding - linear function 증명](https://pozalabs.github.io/transformer/)
- [1] [BERT Positional Encoding 국문 블로그 4](https://wikidocs.net/31379)